{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 7331 - Project 2\n",
    "## Team: Dineen Parker, Dale Legband, Ryan Shuhart\n",
    "collaboration site: https://github.com/rlshuhart/MSDS7331_Project_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### Modules Used ###\n",
    "\n",
    "# Data manipulation: pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization: seaborn and matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# other\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 1 to 30000\n",
      "Data columns (total 26 columns):\n",
      "EDUCATION                     30000 non-null int64\n",
      "PAY_0_Sept                    30000 non-null int64\n",
      "PAY_2_Aug                     30000 non-null int64\n",
      "PAY_3_July                    30000 non-null int64\n",
      "PAY_4_June                    30000 non-null int64\n",
      "PAY_5_May                     30000 non-null int64\n",
      "PAY_6_April                   30000 non-null int64\n",
      "BILL_AMT1_Sept                30000 non-null int64\n",
      "BILL_AMT2_Aug                 30000 non-null int64\n",
      "BILL_AMT3_July                30000 non-null int64\n",
      "BILL_AMT4_June                30000 non-null int64\n",
      "BILL_AMT5_May                 30000 non-null int64\n",
      "BILL_AMT6_April               30000 non-null int64\n",
      "PAY_AMT1_Sept                 30000 non-null int64\n",
      "PAY_AMT2_Aug                  30000 non-null int64\n",
      "PAY_AMT3_July                 30000 non-null int64\n",
      "PAY_AMT4_June                 30000 non-null int64\n",
      "PAY_AMT5_May                  30000 non-null int64\n",
      "PAY_AMT6_April                30000 non-null int64\n",
      "default payment next month    30000 non-null int64\n",
      "AGE_range                     30000 non-null int64\n",
      "LIMIT_BAL_range               30000 non-null int64\n",
      "isMale                        30000 non-null bool\n",
      "Marriage_Married              30000 non-null float64\n",
      "Marriage_Others               30000 non-null float64\n",
      "Marriage_Single               30000 non-null float64\n",
      "dtypes: bool(1), float64(3), int64(22)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "# DL need to modify this to be the shortened version from the previous exercises, \n",
    "# DL but include the data processing\n",
    "# load preprocessed data from previous lab\n",
    "\n",
    "#data_url = \"https://raw.githubusercontent.com/rlshuhart/MSDS7331_Project_1/master/cc_data/cc_data_processed.csv\"\n",
    "data_url = \"../MSDS7331_Project_1/cc_data/cc_data_processed.csv\"\n",
    "\n",
    "cc_data = pd.read_csv(data_url, index_col='ID')\n",
    "cc_data.info(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (15 points total) - part 1\n",
    "* [10 points] Define and prepare your class variables. Use proper variable\n",
    "representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for\n",
    "dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for\n",
    "the analysis.\n",
    "\n",
    "* From our previous projects, the following were converted 'AGE', 'SEX','MARRIAGE', 'LIMIT_BAL': \n",
    "  * One hot encoding of MARRIAGE\n",
    "  * SEX was converted to isMale\n",
    "  * AGE was converted to age ranges\n",
    "  * LIMIT_BAL was converted to limit ranges\n",
    "* We used scaled data (Standard Scaled)\n",
    "* We used a manually created Logistic Model to improve the fit\n",
    "  * Here we will try PCA\n",
    "* We will use StratifiedKFold to create mulitple train/test data sets for cross validation\n",
    "  * We have enough samples that we do not see any weakness of using StratifiedKFold\n",
    "* We will use GridSearch to look for optimal parameters for various models, including visualization\n",
    "of the parameters and their resulting models\n",
    "* We will use Pipelines to match the attribute reduction with the model\n",
    "* We will use ROC Curves and AUC scores to decide on the best model\n",
    "* We will use visulazations throughout, but especially in justyfying our model selection\n",
    "* We will document the most important attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca: [[  7.03060667e-08   1.37321230e-06   1.82097029e-06   1.80072879e-06\n",
      "    1.81882014e-06   1.85172415e-06   1.91629948e-06   4.46548409e-01\n",
      "    4.40869138e-01   4.29188036e-01   3.97481527e-01   3.69467793e-01\n",
      "    3.52658561e-01   2.49803082e-02   2.84029663e-02   2.38468235e-02\n",
      "    1.93466173e-02   1.90055882e-02   2.06290376e-02   2.83173615e-07\n",
      "    6.80505498e-07   8.29640039e-08   8.05617211e-08  -1.55793064e-08\n",
      "   -6.49824147e-08]\n",
      " [  6.04971588e-07  -1.19409232e-06  -1.49848312e-06  -3.80779572e-07\n",
      "    1.26319071e-06   2.67179546e-06   3.68034715e-06  -5.48944059e-01\n",
      "   -3.93810709e-01  -5.03802197e-02   2.57491990e-01   4.24129880e-01\n",
      "    4.79721053e-01   4.36006247e-02   1.69290793e-01   1.41901167e-01\n",
      "    1.00085860e-01   6.20487304e-02  -1.22986756e-02  -2.48042707e-07\n",
      "    5.52403989e-07  -3.54448804e-07  -7.54289769e-09  -3.03961951e-08\n",
      "    3.79390928e-08]\n",
      " [  8.42248801e-07  -3.94776393e-06  -4.20183915e-06  -3.32106245e-06\n",
      "   -1.28040540e-06  -2.27433107e-06  -3.58395924e-06  -1.90289657e-01\n",
      "   -3.58022630e-02   5.38437903e-01   6.86866730e-02  -2.06003561e-01\n",
      "   -3.11888117e-01   2.04356092e-01   6.80280435e-01   4.47277957e-02\n",
      "   -2.51932015e-02   2.77542657e-02   1.42178378e-01   3.78993444e-07\n",
      "    1.23290466e-06  -1.55042435e-08   1.08333975e-07   3.76308680e-08\n",
      "   -1.45964843e-07]\n",
      " [  2.30873154e-06  -8.18071911e-06  -7.86606357e-06  -8.95990886e-06\n",
      "   -9.81818731e-06  -8.51926481e-06  -6.82496538e-06   3.13653674e-01\n",
      "    2.86327260e-02  -3.33158861e-01  -2.47199498e-01  -3.54768054e-03\n",
      "    1.26779325e-01   2.44850515e-01   2.59023163e-01   4.23130877e-01\n",
      "    4.14627883e-01   3.13902298e-01   3.68887641e-01   8.93965782e-07\n",
      "    4.21367317e-06   1.72222127e-07   1.03962123e-07   5.95219127e-08\n",
      "   -1.63484036e-07]]\n",
      "lda: [[  7.59817932e-04   7.04771981e-01   1.46926310e-01   8.78818698e-02\n",
      "    2.50343189e-02   4.20094791e-02   1.13313983e-02  -4.85439634e-06\n",
      "    1.23761406e-06   1.97318184e-07  -5.43432670e-07  -1.94950653e-07\n",
      "    8.50393685e-07  -5.61470804e-06  -1.63818807e-06  -3.64871337e-07\n",
      "   -2.07754775e-06  -2.78994845e-06  -9.93940048e-07   6.68782782e-02\n",
      "   -3.25080364e-02   1.19976250e-01   1.00536155e-01  -1.12246768e-01\n",
      "   -9.45517419e-02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "cc_data_target = cc_data['default payment next month'].values\n",
    "cc_data_Xs = cc_data.drop('default payment next month', axis='columns').values\n",
    "\n",
    "# Column names for labeling\n",
    "cc_data_Xs_names = cc_data.drop('default payment next month', axis='columns').columns\n",
    "\n",
    "#PCA or LDA code\n",
    "# now let's use PCA, and LDA to find the two \"best\" dimensions of this data\n",
    "# these are linear transforms to help project the features into something more understandable\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.lda import LDA\n",
    "\n",
    "#copy the data to use code from sample\n",
    "X = cc_data_Xs\n",
    "y = cc_data_target\n",
    "target_names = cc_data_Xs_names\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "X_pca = pca.fit(X).transform(X) # fit data and then transform it\n",
    "\n",
    "lda = LDA(n_components=4)\n",
    "X_lda = lda.fit(X, y).transform(X) # fit data and then transform it\n",
    "\n",
    "# print the components\n",
    "\n",
    "print ('pca:', pca.components_)\n",
    "print ('lda:', lda.scalings_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00*EDUC +0.00*PAY_0 +0.00*PAY_ +0.00*PAY_3 +0.00*PAY_4 +0.00*PAY_ +0.00*PAY_6_ +0.45*BILL_AMT1 +0.44*BILL_AMT +0.43*BILL_AMT3 +0.40*BILL_AMT4 +0.37*BILL_AMT +0.35*BILL_AMT6_ +0.02*PAY_AMT1 +0.03*PAY_AMT +0.02*PAY_AMT3 +0.02*PAY_AMT4 +0.02*PAY_AMT +0.02*PAY_AMT6_ +0.00*AGE_ +0.00*LIMIT_BAL_ +0.00*i +0.00*Marriage_Ma -0.00*Marriage_O -0.00*Marriage_S \n",
      "0.00*EDUC -0.00*PAY_0 -0.00*PAY_ -0.00*PAY_3 +0.00*PAY_4 +0.00*PAY_ +0.00*PAY_6_ -0.55*BILL_AMT1 -0.39*BILL_AMT -0.05*BILL_AMT3 +0.26*BILL_AMT4 +0.42*BILL_AMT +0.48*BILL_AMT6_ +0.04*PAY_AMT1 +0.17*PAY_AMT +0.14*PAY_AMT3 +0.10*PAY_AMT4 +0.06*PAY_AMT -0.01*PAY_AMT6_ -0.00*AGE_ +0.00*LIMIT_BAL_ -0.00*i -0.00*Marriage_Ma -0.00*Marriage_O +0.00*Marriage_S \n",
      "0.00*EDUC -0.00*PAY_0 -0.00*PAY_ -0.00*PAY_3 -0.00*PAY_4 -0.00*PAY_ -0.00*PAY_6_ -0.19*BILL_AMT1 -0.04*BILL_AMT +0.54*BILL_AMT3 +0.07*BILL_AMT4 -0.21*BILL_AMT -0.31*BILL_AMT6_ +0.20*PAY_AMT1 +0.68*PAY_AMT +0.04*PAY_AMT3 -0.03*PAY_AMT4 +0.03*PAY_AMT +0.14*PAY_AMT6_ +0.00*AGE_ +0.00*LIMIT_BAL_ -0.00*i +0.00*Marriage_Ma +0.00*Marriage_O -0.00*Marriage_S \n",
      "0.00*EDUC -0.00*PAY_0 -0.00*PAY_ -0.00*PAY_3 -0.00*PAY_4 -0.00*PAY_ -0.00*PAY_6_ +0.31*BILL_AMT1 +0.03*BILL_AMT -0.33*BILL_AMT3 -0.25*BILL_AMT4 -0.00*BILL_AMT +0.13*BILL_AMT6_ +0.24*PAY_AMT1 +0.26*PAY_AMT +0.42*PAY_AMT3 +0.41*PAY_AMT4 +0.31*PAY_AMT +0.37*PAY_AMT6_ +0.00*AGE_ +0.00*LIMIT_BAL_ +0.00*i +0.00*Marriage_Ma +0.00*Marriage_O -0.00*Marriage_S \n",
      "['0.00*EDUC +0.70*PAY_0 +0.15*PAY_ +0.09*PAY_3 +0.03*PAY_4 +0.04*PAY_ +0.01*PAY_6_ -0.00*BILL_AMT1 +0.00*BILL_AMT +0.00*BILL_AMT3 -0.00*BILL_AMT4 -0.00*BILL_AMT +0.00*BILL_AMT6_ -0.00*PAY_AMT1 -0.00*PAY_AMT -0.00*PAY_AMT3 -0.00*PAY_AMT4 -0.00*PAY_AMT -0.00*PAY_AMT6_ +0.07*AGE_ -0.03*LIMIT_BAL_ +0.12*i +0.10*Marriage_Ma -0.11*Marriage_O -0.09*Marriage_S ']\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "# this function definition just formats the weights into readable strings\n",
    "# you can skip it without loss of generality to the Data Science content\n",
    "def get_feature_names_from_weights(weights, names):\n",
    "    tmp_array = []\n",
    "    for comp in weights:\n",
    "        tmp_string = ''\n",
    "        for fidx,f in enumerate(names):\n",
    "            if fidx>0 and comp[fidx]>=0:\n",
    "                tmp_string+='+'\n",
    "            tmp_string += '%.2f*%s ' % (comp[fidx],f[:-5])\n",
    "        tmp_array.append(tmp_string)\n",
    "    return tmp_array\n",
    "  \n",
    "# now let's get to the Data Analytics!\n",
    "pca_weight_strings = get_feature_names_from_weights(pca.components_, cc_data_Xs_names) \n",
    "lda_weight_strings = get_feature_names_from_weights(lda.scalings_.T, cc_data_Xs_names) \n",
    "\n",
    "# create some pandas dataframes from the transformed outputs\n",
    "df_pca = pd.DataFrame(X_pca,columns=[pca_weight_strings])\n",
    "df_lda = pd.DataFrame(X_lda,columns=[lda_weight_strings])\n",
    "\n",
    "from pandas.tools.plotting import scatter_plot\n",
    "\n",
    "print(pca_weight_strings[0])\n",
    "print(pca_weight_strings[1])\n",
    "print(pca_weight_strings[2])\n",
    "print(pca_weight_strings[3])\n",
    "print(lda_weight_strings)\n",
    "# plot is broken.\n",
    "# scatter plot the output, with the names created from the weights\n",
    "ax = scatter_plot(df_pca, pca_weight_strings[0], pca_weight_strings[1])\n",
    "newfig = plt.figure()\n",
    "#ax = scatter_plot(df_lda, 'x', 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (15 points total) - part 2\n",
    "\n",
    "* [5 points] Describe the final dataset that is used for classification/regression (include a\n",
    "description of any newly formed variables you created).\n",
    " * desribe PCA result\n",
    "\n",
    "<font color='Red'>Not Complete</font>\n",
    "\n",
    "\n",
    "#### Convert data to numpy arrays for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc_data_target = cc_data['default payment next month']\n",
    "cc_data_Xs = cc_data.drop('default payment next month', axis='columns')\n",
    "\n",
    "# Column names for labeling\n",
    "cc_data_Xs_names = cc_data.drop('default payment next month', axis='columns').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Evaluation (70 points total)\n",
    "* [10 points] Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s)\n",
    "appropriate for analyzing the results of your modeling? Give a detailed explanation\n",
    "backing up any assertions.\n",
    "\n",
    "<font color='Red'>Not Complete</font>\n",
    "\n",
    "    ROC Curves and AUC scores are good measures for imbalanced data scources. \"The main goal for learning from imbalanced datasets is to improve the recall without hurting the precision...The F-value metric is one measure that combines the trade-offs of precision and recall, and outputs a single number reflecting the \"goodness\" of a classifier in the presence of rare classes. \"\n",
    "    https://www3.nd.edu/~dial/publications/chawla2005data.pdf\n",
    "\n",
    "    From Wikipedia:\n",
    "    \"Precision (also called positive predictive value) is the fraction of retrieved instances that are relevant\"\n",
    "    \"Recall (also known as sensitivity) is the fraction of relevant instances that are retrieved.\"\n",
    "    https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "* [10 points] Choose the method you will use for dividing your data into training and\n",
    "testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why\n",
    "your chosen method is appropriate or use more than one method as appropriate. For\n",
    "example, if you are using time series data then you should be using continuous training\n",
    "and testing sets across time.\n",
    "\n",
    "<font color='Red'>Not Complete</font>\n",
    "\n",
    "\n",
    "* [20 points] Create three different classification/regression models for each task (e.g.,\n",
    "random forest, KNN, and SVM for task one and the same or different algorithms for\n",
    "task two). Two modeling techniques must be new (but the third could be SVM or\n",
    "logistic regression). Adjust parameters as appropriate to increase generalization\n",
    "performance using your chosen metric. You must investigate different parameters\n",
    "of the algorithms!\n",
    "\n",
    "<font color='Red'>Not Complete</font>\n",
    "\n",
    "\n",
    "* [10 points] Analyze the results using your chosen method of evaluation. Use\n",
    "visualizations of the results to bolster the analysis. Explain any visuals and analyze why\n",
    "they are interesting to someone that might use this model.\n",
    "\n",
    "<font color='Red'>Not Complete</font>\n",
    "\n",
    "\n",
    "* [10 points] Discuss the advantages of each model for each classification task, if any. If\n",
    "there are not advantages, explain why. Is any model better than another? Is the\n",
    "difference significant with 95% confidence? Use proper statistical comparison methods.\n",
    "You must use statistical comparison techniques—be sure they are appropriate for your\n",
    "chosen method of validation as discussed in unit 7 of the course.\n",
    "\n",
    "<font color='Red'>Not Complete</font>\n",
    "\n",
    "\n",
    "* [10 points] Which attributes from your analysis are most important? Use proper\n",
    "methods discussed in class to evaluate the importance of different attributes. Discuss\n",
    "the results and hypothesize about why certain attributes are more important than others\n",
    "for a given classification task.\n",
    "\n",
    "<font color='Red'>Not Complete</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployment (5 points total)\n",
    "* How useful is your model for interested parties (i.e., the companies or organizations\n",
    "that might want to use it for prediction)? How would you measure the model's value if it\n",
    "was used by these parties? How would your deploy your model for interested parties?\n",
    "What other data should be collected? How often would the model need to be updated,\n",
    "etc.?\n",
    "\n",
    "<font color='Red'>Not Complete</font>\n",
    "* The models provide a starting point for interested parties, identifying the most fruitful models with the data available. From this point, a conversation would be in order with the prospective model users about additional data that would enhance predictions and cover corner cases, and request additional data to test and validate again. \n",
    "* To measure the value, one would implement the model in a business setting. This is envisioned as  customer advisers calling customers that are going to default to offer help or whatever action the business sees fit to take to avoid that outcome. At the point where the CC company has fewer defaults and people have fewer defaults to struggle with, then you have a valid measurement: old default rate versus new default rate.\n",
    "* Updates to the model would be determined by its overall impact to the customer and company, combined with new ideas that develop as the model is tweaked to come up with better predictions. At some point the tweaks will not significantly improve the prediction at which time the company may decide to just keep it as-is and move the resource to more productive problems that need to be tackled. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exceptional Work (10 points total)\n",
    "* You have free reign to provide additional analyses.\n",
    "* One idea: grid search parameters in a parallelized fashion and visualize the\n",
    "performances across attributes. Which parameters are most significant for making a\n",
    "good model for each classification algorithm?\n",
    "\n",
    "<font color='Red'>Not Complete</font>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
